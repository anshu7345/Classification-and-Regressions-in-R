---
title: "MLA_CIA4"
output:
  word_document: default
  html_document: default
date: "2023-09-17"
---

Name: Anshu Singh Roll no: 22121104

```{r}
getOption("repos")
options(repos = "https://cran.r-project.org")
```

```{r}
install.packages("knitr")
```

```{r}
library(knitr)
```

```{r}
library(dplyr)
```

```{r}
library(tidyverse)
```

```{r}
#install.packages("readxl")
library(readxl)
```

```{r}
setwd("C:/Users/Phoni/Downloads")
data = read.csv("MLA CIA - 4.csv")
data
```

```{r}
sample(data)
```

```{r}
duplicated(data)
```

```{r}
data=na.omit(data)
```

```{r}
is.na(data)
```

```{r}
library(ggplot2)
```

```{r}
# Check for missing values
sum(is.na(data))

# Remove rows with missing values
data_clean <- data[complete.cases(data), ]
data_clean

# Drop the  column and save the modified data frame into a new variable
data6 <- data_clean[, -which(names(data_clean) == "Cust_ID")]
data6
```

```{r}
summary(data_clean)
```

```{r}
data = na.omit(data)

sum(is.na(data))
```

```{r}
# Univariant analysis of every variabes

ggplot(data_clean, aes(x = Target)) +
  geom_histogram(binwidth = 0.5, fill = "#e6e6fa", color = "black") +
  labs(x = "No. of Articles")

ggplot(data_clean, aes(x = Age)) +
  geom_histogram(binwidth = 0.5, fill = "#e6e6fa", color = "black") +
  labs(x = "Age Distribution")

ggplot(data_clean, aes(x = Gender)) +
  geom_bar(fill = "#e6e6fa",color = "black") +
  labs(title = "Gender Distribution", x = "Gender", y = "Count")

ggplot(data_clean, aes(x = Balance)) +
  geom_histogram(fill = "#e6e6fa", color = "black") +
  labs(x = "Balance Distribution")

hist(data_clean$Balance, xlab = "Balance",
     col = "#e6e6fa", border = "black")


ggplot(data_clean, aes(x = data_clean$Occupation)) +
  geom_bar(fill = "#e6e6fa", color = "black") +
  labs(title = "Occupation", x = "Occupation", y = "Count")

ggplot(data_clean, aes(x = No_OF_CR_TXNS)) +
  geom_histogram(binwidth = 0.5, fill = "#e6e6fa", color = "black") +
  labs(x = "No_OF_CR_TXNS")

ggplot(data_clean, aes(x = data_clean$AGE_BKT)) +
  geom_bar(fill = "#e6e6fa", color = "black") +
  labs(title = "AGE_BKT", x = "AGE_BKT", y = "Count")

ggplot(data_clean, aes(x = Holding_Period)) +
  geom_histogram(binwidth = 0.5, fill = "#e6e6fa", color = "black") +
  labs(x = "Holding_Period")

ggplot(data_clean, aes(x = SCR)) +
  geom_histogram(fill = "#e6e6fa", color = "black") +
  labs(x = "SCR")
```

```{r}
correlation_matrix <- cor(data6[, c("Age", "Balance", "SCR", "Holding_Period")])
print(correlation_matrix)

```

```{r}
# Scatter plot between Age and Balance
ggplot(data6, aes(x = Age, y = Balance)) +
  geom_point() +
  labs(title = "Scatter Plot: Age vs. Balance", x = "Age", y = "Balance")

```

```{r}
# Grouped bar chart for Gender vs. Target
ggplot(data6, aes(x = Gender, fill = Target)) +
  geom_bar(position = "dodge") +
  labs(title = "Gender vs. Target", x = "Gender", fill = "Target")

```

```{r}
# Scatter plot between No_OF_CR_TXNS and SCR
ggplot(data6, aes(x = No_OF_CR_TXNS, y = SCR)) +
  geom_point() +
  labs(title = "Scatter Plot: No_OF_CR_TXNS vs. SCR", x = "No_OF_CR_TXNS", y = "SCR")

```

```{r}
# Grouped bar chart for Occupation vs. Target
ggplot(data6 , aes(x = Occupation, fill = Target)) +
  geom_bar(position = "dodge") +
  labs(title = "Occupation vs. Target", x = "Occupation", fill = "Target") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability

```

```{r}
# Grouped bar chart for Occupation vs. Target
ggplot(data_clean, aes(x = AGE_BKT, fill = Target)) +
  geom_bar(position = "dodge") +
  labs(title = "Occupation vs. Target", x = "AGE_BKT", fill = "Target") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

## Stepwaise Regration

Forword Regration

```{r}
full_model=lm(Target ~  ., data = data6)
model1=step(full_model, direction = "forward")
model1
```

Backword Regration

```{r}
full_model=lm(Target ~  ., data = data6)
model2=step(full_model, direction = "backward")
model2
```

```{r}
library('boot')
```

cross validation

```{r}
model <- glm(Target ~ ., data=data6)
cv.error <- cv.glm(data6, model, K = 11)
print(cv.error$delta)
```

Regularization model

```{r}
library(glmnet)
```

```{r}
# prepare matrix of predictors and response variable
x <- model.matrix(Target ~ ., data6)[,-1] 
# remove intercept column
y <- data6$Target
# fit ridge regression model
ridge_model <- glmnet(x, y, alpha = 0)
# fit lasso regression model
lasso_model <- glmnet(x, y, alpha = 1)
# print models
print(ridge_model)
print(lasso_model)
```

```{r}
library(boot)
```

Cross Validation

```{r}
# perform cross-validation for ridge model
cv.ridge <- cv.glmnet(x, y, alpha = 0)
# perform cross-validation for lasso model
cv.lasso <- cv.glmnet(x, y, alpha = 1)
# print optimal lambda values
print(cv.ridge$lambda.min)
print(cv.lasso$lambda.min)
```

Decision Tree ID3

```{r}



# ID3


# Load necessary libraries
library(rpart)
library(rpart.plot)
library(caret)

# Split the data into training and testing sets
set.seed(123)

data=data_clean[-1]

trainIndex <- createDataPartition(data$Target, p = .7, list = FALSE, times = 1)

dataTrain <- data[ trainIndex,]
dataTest <- data[-trainIndex,]

data$Target=as.factor(data$Target)

# Fit the ID3 model (technically, rpart uses the CART algorithm, which is similar to ID3)
id3_model <- rpart(Target ~ ., data = dataTrain,method = "class")

id3_model
# Visualize the decision tree
rpart.plot(id3_model, extra = 106)

# Predict on the test data
predictions <- predict(id3_model, dataTest, type = "class")
dataTest$Target=as.factor(dataTest$Target)
# Evaluate the model
confusionMatrix(predictions, dataTest$Target)
#confusionMatrix(as.factor(predictions),dataTest$Target)
```

```{r}
summary(id3_model)
```

```{r}
library(pROC)
```

C5.0

```{r}
library(C50)
library(caret)
```

```{r}
set.seed(123)
trainIndex1 <- createDataPartition(data$Target, p = .7, list = FALSE,times = 1) 
dataTrain1 <- data[ trainIndex1,]
dataTes1t <- data[-trainIndex1,]
```

```{r}

c50_m <- C5.0(Target ~  ., data = dataTrain1, method = "class")
```

```{r}
summary(c50_m)
```

```{r}

levels(dataTest$Target)
```

```{r}
predictions <- predict(c50_m, data)
```

```{r}
confusionMatrix(predictions, data$Target)
```

CART

```{r}
library(rpart)
library(rpart.plot)
library(caret)
```

```{r}
set.seed(123)

data = data_clean[,-1]

trainIndex1 <- createDataPartition(data$Target, p = .7, list = FALSE, times = 1)

datatrain <- data[ trainIndex1,]
datatest <- data[-trainIndex1,]

# Build the CART model
model1 <- rpart(Target ~ ., data = datatrain, method = "class")
# Print a summary of the model
print(model1)
# Predict on the test data
predictions <- predict(model1, data, type = "class")
#datatest$Target=as.factor(datatest$Target)
#confusionMatrix(predictions, datatest$Target)

# Evaluate the model
confusionMatrix(predictions, as.factor(data$Target))
```

```{r}

rpart.plot(model1, extra = 106)
```

LOgistic Regrassion

```{r}
# Load necessary libraries (if not already loaded)
library(dplyr)
library(broom)
data1 = data_clean[,-1]
# Load your dataset (replace 'your_dataset.csv' with your dataset file)
# data <- read.csv("your_dataset.csv")

# Perform binary logistic regression
model <- glm(Target ~ ., data = data1, family = "binomial")

# Summary of the logistic regression model
summary(model)

# Optionally, tidy the model summary using the broom package
tidy_summary <- tidy(model)
print(tidy_summary)

```

```{r}
exp(coef(model))
```

```{r}
#confution matrix
predicted_probs <- predict(model, type = "response")

predicted_classes <- ifelse(predicted_probs > 0.5, "0", "1")

confusionMatrix(predictions, as.factor(data$Target))
```

```{r}
library(pROC)

# Create ROC curve
roc_obj1 <- roc(as.factor(data1$Target), predicted_probs)
plot(roc_obj1, main = "ROC Curve", col="red")
auc(roc_obj1)
```

```{r}
library(e1071)
library(pROC)
```

# NAIVE BAYES

```{r}
# NAIVE BAYES

# Required library
library(e1071)

set.seed(123)
data2=data_clean[-1]
train_indices <- sample(1:nrow(data2),0.7*nrow(data2))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]

nb_model <- naiveBayes(data2$Target ~ ., data = data2)

summary(nb_model)

#Predict the outcomes on the testing set:
predictions_nb <- predict(nb_model, test_data)


# Compare the predicted outcomes with the actual outcomes
confusion_matrix <- table(test_data$Target,predictions_nb)
print(confusion_matrix)


probabilities_nb <- predict(nb_model,test_data, type = "raw")

roc_obj <- roc(test_data$Target,probabilities_nb[,2], levels = c("0", "1"))
plot(roc_obj, main="ROC Curve for Naive BayesModel",col="blue")

auc(roc_obj)
# Get the coefficients for the Naive Bayes model
coefficients_nb <- nb_model$apriori


# The coefficients represent the conditional probabilities for each class and predictor variable
# coefficients_nb contains the prior probabilities of each class
# conditional_probabilities contains the conditional probabilities for each predictor variable

# Print the coefficients
print(coefficients_nb)



```

```{r}
confusionMatrix(factor(test_data$Target),factor(predictions_nb))
```

SVM

```{R}
data3=data_clean[-1]
train_indices <- sample(1:nrow(data3),0.7*nrow(data3))
train_data <- data3[train_indices, ]
test_data <- data3[-train_indices, ]
```

```{R}

library(e1071)
svm_model <- svm(Target ~ ., data = train_data, kernel = "radial",
cost = 10, scale = TRUE)
```

```{R}
predictions_svm <- predict(svm_model,test_data)
```

```{R}
confusion_matrix <- table(test_data$Target,
predictions_svm)

```

```{r}
# Get decision values
decision_values <-attributes(predict(svm_model, test_data,decision.values = TRUE))$decision.values
roc_obj <- roc(test_data$Target,
decision_values, levels = c("0", "1"))
plot(roc_obj, main="ROC Curve for SVM Model")
```

```{r}


confusionMatrix(factor(test_data$Target),factor(predictions_svm),levels= c("0", "1"))
```

The area under the ROC curve (AUC) can be a good metric to evaluate the model's performance:

```{R}
auc(roc_obj)
```


```{r}
library(shiny)
library(rpart)
library(rpart.plot)
library(caret)


```

```{r}
# Load required libraries
library(shiny)

# Define UI
ui <- fluidPage(
  titlePanel(" Probablity of willing to Buy or Not"),
  sidebarLayout(
    sidebarPanel(
      # Input widgets for the predictor variables
      numericInput("age", "Age:", value = 30, min = 18, max = 100),
      selectInput("gender", "Gender:", choices = c("Male", "Female", "Other")),
      numericInput("balance", "Balance:", value = 1000, min = 0),
      selectInput("occupation", "Occupation:", choices = c("Salaried", "Self-Employed", "Business")),
      numericInput("no_of_cr_txns", "Number of Credit Transactions:", value = 5, min = 0),
      numericInput("holding_period", "Holding Period (months):", value = 12, min = 0),
      numericInput("scr", "SCR:", value = 700, min = 300, max = 900),
      actionButton("process_button", "Process") # Add the "Process" button
    ),
    mainPanel(
      # Display the prediction result
      verbatimTextOutput("prediction_result")
    )
  )
)

# Define the server logic for the Shiny app
server <- function(input, output) {
  
  # Observe the "Process" button click and make predictions
  observeEvent(input$process_button, {
    # Load the logistic regression coefficients
    coefficients <- c(
      Intercept = 0.04846295,
      Age = 0.94887616,
      GenderM = 0.63974453,
      GenderO = 0.76492438,
      Balance = 0.999995476,
      OccupationSAL = 0.54610458,
      OccupationSELF_EMP = 2.27070460,
      OccupationSENP = 0.24633821,
      No_OF_CR_TXNS = 1.03098173,
      AGE_BKT50 = 2.50905329,
      AGE_BKT2630 = 0.86492474,
      AGE_BKT3135 = 1.34612159,
      AGE_BKT3640 = 1.85851927,
      AGE_BKT4145 = 4.05217474,
      AGE_BKT4650 = 2.80551196,
      Holding_Period = 0.87159519,
      SCR = 1.00419594
    ) 
    
    # Define a function to calculate the log-odds
    calculate_log_odds <- function(input_data) {
      log_odds <- coefficients["Intercept"]
      for (var_name in names(input_data)) {
        log_odds <- log_odds + coefficients[var_name] * input_data[[var_name]]
      }
      return(log_odds)
    }
    
    # Define a function to make predictions
    make_prediction <- function(log_odds) {
      probability <- 1 / (1 + exp(-log_odds))
      return(ifelse(probability >= 0.5, "Yes", "No"))
    }
    
    # Create a data frame from user input
    input_data <- data.frame(
      Age = input$age,
      GenderM = as.integer(input$gender == "Male"),
      GenderO = as.integer(input$gender == "Other"),
      Balance = input$balance,
      OccupationSAL = as.integer(input$occupation == "Salaried"),
      OccupationSELF_EMP = as.integer(input$occupation == "Self-Employed"),
      OccupationSENP = as.integer(input$occupation == "Business"),
      No_OF_CR_TXNS = input$no_of_cr_txns,
      AGE_BKT50 = as.integer(input$age > 50),
      AGE_BKT2630 = as.integer(input$age >= 26 && input$age <= 30),
      AGE_BKT3135 = as.integer(input$age >= 31 && input$age <= 35),
      AGE_BKT3640 = as.integer(input$age >= 36 && input$age <= 40),
      AGE_BKT4145 = as.integer(input$age >= 41 && input$age <= 45),
      AGE_BKT4650 = as.integer(input$age >= 46 && input$age <= 50),
      Holding_Period = input$holding_period,
      SCR = input$scr
    )
    
    # Calculate the log-odds and make predictions
    log_odds <- calculate_log_odds(input_data)
    prediction <- make_prediction(log_odds)
    
    # Display the prediction result
    output$prediction_result <- renderText({
      paste("Predicted Outcome:", prediction)
    })
  })
}

# Run the Shiny app
shinyApp(ui, server)

```
